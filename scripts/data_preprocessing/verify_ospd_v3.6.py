#!/usr/bin/env python3
"""Verify reproducibility of vLLM inference against an all.jsonl file
(generated by collate_ospd_v3.6_subsets.py).

The script:
  1. Reads records from INPUT_PATH (all.jsonl).
  2. Keeps up to MAX_SAMPLES records that have non-null raw outputs.
  3. Re-runs inference TWICE (run1, run2) in SEPARATE SUBPROCESSes (each
     with a fresh vLLM engine) to test cross-init reproducibility.
  4. Performs three pairwise comparisons: run1↔orig, run2↔orig, run1↔run2.
  5. For each pair reports both full-output matching and extracted-answer
     matching (for w_hint and wo_hint separately).

Usage:
  # Default: orchestrator mode — spawns run1 & run2 subprocesses, then compares.
  python verify_ospd_v3.6.py

  # Single-run mode (used internally by the orchestrator):
  python verify_ospd_v3.6.py --mode infer --output /path/to/run.jsonl
"""
import argparse
import json
import os
import re
import subprocess
import sys
from typing import Any, Dict, Iterable, List, Optional, Sequence


# ── configuration ──────────────────────────────────────────────────────
INPUT_PATH = "/home/efs/hardychen/workspaces/gptoss_rlp/verl-internvl/datasets/rlp/v3.6/vlaa_thinking-geoqa170k_reasoning/all.jsonl"
MODEL_PATH = "/home/efs/hardychen/models/InternVL3_5-4B-Pretrained"
MAX_SAMPLES = 20
MAX_NEW_TOKENS = 1024
TEMPERATURE = 0.0
TOP_P = 1.0
TOP_K = -1
# NOTE: The original generation used tensor_parallel_size=8 (default in
# collate_ospd_v3.6_subsets.py).  However TP>1 is NON-DETERMINISTIC across
# different process invocations due to NCCL allreduce floating-point ordering.
# To get exact full-output reproducibility you MUST use TP=1.  The tradeoff
# is that TP=1 results won't match the original TP=8 outputs either, but
# at least run1↔run2 AND run↔orig will be fully deterministic if the
# original was also generated with TP=1.
TENSOR_PARALLEL_SIZE = 8
GPU_MEMORY_UTILIZATION = 0.9
BATCH_SIZE = 100              # match original: collate_ospd_v3.6_subsets.py default=100
SEED = 0                      # match original: collate_ospd_v3.6_subsets.py uses seed=0
RUN1_OUTPUT = "/home/efs/hardychen/workspaces/gptoss_rlp/verl-internvl/datasets/rlp/debug_ospd/verify_run1.jsonl"
RUN2_OUTPUT = "/home/efs/hardychen/workspaces/gptoss_rlp/verl-internvl/datasets/rlp/debug_ospd/verify_run2.jsonl"

_SYSTEM_PROMPT = (
    "You are an AI assistant that rigorously follows this response "
    "protocol:\n\n1. First, conduct a detailed analysis of the "
    "question. Consider different angles, potential solutions, and "
    "reason through the problem step-by-step. Enclose this entire "
    "thinking process within <think>*</think> tags.\n\n2. After "
    "the thinking section, provide a clear, concise, and direct answer "
    r"to the user's question within \boxed{}."
    "\n\n"
    r"Output format: <think>[thinking process]</think>\boxed{[answer]}"
)


# ── shared helpers ─────────────────────────────────────────────────────
def _iter_jsonl(path: str) -> Iterable[Dict[str, Any]]:
    with open(path, "r", encoding="utf-8") as handle:
        for line in handle:
            line = line.strip()
            if not line:
                continue
            yield json.loads(line)


def _build_user_prompt(question: str, hint: str, include_image_token: bool = True) -> str:
    hint_block = f"Hint: {hint}\n" if hint else ""
    image_token = "<image>\n" if include_image_token else ""
    return f"{image_token}{hint_block}{question.replace('<image>', '')}\n"


def _build_messages(question: str, hint: str) -> List[Dict[str, str]]:
    user_prompt = _build_user_prompt(question, hint)
    return [
        {"content": _SYSTEM_PROMPT, "role": "system"},
        {"content": user_prompt, "role": "user"},
    ]


def _parse_boxed_answer(text: str) -> Optional[str]:
    if not text:
        return None
    match = re.search(r"<think>.*?</think>.*?\\boxed\{(.+?)\}\s*", text, re.S)
    if not match:
        return None
    return match.group(1).strip()


def _normalize_answer(text: str) -> str:
    normalized = str(text).strip().lower()
    if normalized.startswith("$") and normalized.endswith("$"):
        normalized = normalized[1:-1].strip()
    return normalized


def _is_correct(pred: Optional[str], answers: Sequence[str]) -> bool:
    if pred is None:
        return False
    pred_norm = _normalize_answer(pred)
    for answer in answers:
        if pred_norm == _normalize_answer(answer):
            return True
    return False


def _select_samples() -> List[Dict[str, Any]]:
    """Read INPUT_PATH and return up to MAX_SAMPLES records with stored outputs."""
    samples: List[Dict[str, Any]] = []
    for record in _iter_jsonl(INPUT_PATH):
        extra = record.get("extra_info") or {}
        if extra.get("raw_output_w_hint") is not None and extra.get("raw_output_wo_hint") is not None:
            samples.append(record)
            if len(samples) >= MAX_SAMPLES:
                break
    return samples


# ═══════════════════════════════════════════════════════════════════════
# MODE: infer  — run a single inference pass (called in a subprocess)
# ═══════════════════════════════════════════════════════════════════════
def _run_infer(output_path: str) -> None:
    """Initialize vLLM, run inference, write results to output_path."""
    # Heavy imports only in the subprocess that needs them
    os.environ["VLLM_USE_V1"] = "0"
    from PIL import Image
    from transformers import AutoTokenizer
    from vllm import LLM, SamplingParams

    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    sampling_params = SamplingParams(
        temperature=TEMPERATURE,
        top_p=TOP_P,
        top_k=TOP_K,
        max_tokens=MAX_NEW_TOKENS,
        seed=SEED,
    )
    llm = LLM(
        model=MODEL_PATH,
        trust_remote_code=True,
        dtype="float16",
        tensor_parallel_size=TENSOR_PARALLEL_SIZE,
        gpu_memory_utilization=GPU_MEMORY_UTILIZATION,
        # ── Match training vLLM engine settings for reproducibility ──
        disable_custom_all_reduce=True,
        enable_chunked_prefill=False,
        enforce_eager=False,
    )

    # ── select & load samples ──
    samples = _select_samples()
    if not samples:
        print("No samples with stored raw outputs found.")
        return

    loaded: List[Dict[str, Any]] = []
    for sample in samples:
        extra = sample.get("extra_info") or {}
        image_paths = extra.get("image_paths") or sample.get("images") or sample.get("image_paths") or []
        image_path = image_paths[0] if image_paths else ""
        if not image_path or not os.path.exists(image_path):
            continue
        try:
            sample["_image"] = Image.open(image_path).convert("RGB")
        except Exception:
            continue
        loaded.append(sample)

    if not loaded:
        print("No loadable samples found.")
        return

    print(f"[infer] {len(loaded)} samples loaded, running inference …")

    # ── batch inference ──
    def _build_prompt_text(question: str, hint: str) -> str:
        messages = _build_messages(question, hint)
        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    def _run_batch(batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        prompts: List[Dict[str, Any]] = []
        for item in batch:
            extra = item.get("extra_info") or {}
            question = extra.get("question", "")
            hint = extra.get("hint") or ""
            prompt_a = _build_prompt_text(question, hint)
            prompt_b = _build_prompt_text(question, "")
            prompts.append({"prompt": prompt_a, "multi_modal_data": {"image": item["_image"]}})
            prompts.append({"prompt": prompt_b, "multi_modal_data": {"image": item["_image"]}})
        outputs_raw = llm.generate(prompts, sampling_params)
        outputs = [output.outputs[0].text if output.outputs else "" for output in outputs_raw]

        results: List[Dict[str, Any]] = []
        for i, item in enumerate(batch):
            output_a = outputs[2 * i] if 2 * i < len(outputs) else ""
            output_b = outputs[2 * i + 1] if 2 * i + 1 < len(outputs) else ""
            answer_a = _parse_boxed_answer(output_a)
            answer_b = _parse_boxed_answer(output_b)
            extra = item.get("extra_info") or {}
            answers = extra.get("answer") or extra.get("answers") or (item.get("reward_model") or {}).get("ground_truth") or []
            question = extra.get("question", "")
            hint = extra.get("hint") or ""
            prompt_wh = _build_prompt_text(question, hint)
            prompt_woh = _build_prompt_text(question, "")
            results.append(
                {
                    "uid": extra.get("uid"),
                    "question": question,
                    "hint": hint,
                    "answers": answers,
                    "prompt_w_hint": prompt_wh,
                    "prompt_wo_hint": prompt_woh,
                    "raw_output_w_hint": output_a,
                    "raw_output_wo_hint": output_b,
                    "answer_w_hint": answer_a,
                    "answer_wo_hint": answer_b,
                    "w_h_c": _is_correct(answer_a, answers),
                    "wo_h_c": _is_correct(answer_b, answers),
                }
            )
        return results

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    pending: List[Dict[str, Any]] = []
    with open(output_path, "w", encoding="utf-8") as handle:
        for sample in loaded:
            pending.append(sample)
            if len(pending) >= max(1, BATCH_SIZE):
                for result in _run_batch(pending):
                    handle.write(json.dumps(result, ensure_ascii=False) + "\n")
                pending = []
        if pending:
            for result in _run_batch(pending):
                handle.write(json.dumps(result, ensure_ascii=False) + "\n")

    print(f"[infer] wrote results → {output_path}")


# ═══════════════════════════════════════════════════════════════════════
# MODE: compare  — read run1, run2, orig and do pairwise comparison
# ═══════════════════════════════════════════════════════════════════════
def _load_results(path: str) -> List[Dict[str, Any]]:
    return list(_iter_jsonl(path))


def _extract_orig() -> List[Dict[str, Any]]:
    """Build a results-like list from the original stored outputs in all.jsonl.

    If the original all.jsonl already contains prompt_w_hint / prompt_wo_hint
    (logged by collate_ospd_v3.6_subsets.py), use those directly.  Otherwise
    reconstruct the prompt strings from the stored question/hint so we can
    still compare.
    """
    from transformers import AutoTokenizer as _AT

    tokenizer = _AT.from_pretrained(MODEL_PATH, trust_remote_code=True)

    def _orig_prompt_text(question: str, hint: str) -> str:
        messages = _build_messages(question, hint)
        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    samples = _select_samples()
    # Also need to filter by loadable images (same as infer does)
    from PIL import Image as _PIL_Image
    orig: List[Dict[str, Any]] = []
    for sample in samples:
        extra = sample.get("extra_info") or {}
        image_paths = extra.get("image_paths") or sample.get("images") or sample.get("image_paths") or []
        image_path = image_paths[0] if image_paths else ""
        if not image_path or not os.path.exists(image_path):
            continue
        try:
            _PIL_Image.open(image_path).verify()
        except Exception:
            continue
        answers = extra.get("answer") or extra.get("answers") or (sample.get("reward_model") or {}).get("ground_truth") or []
        question = extra.get("question") or ""
        hint = extra.get("hint") or ""
        # Prefer stored prompts (logged by collate script); fall back to reconstruction
        stored_pw = extra.get("prompt_w_hint")
        stored_pwo = extra.get("prompt_wo_hint")
        orig.append(
            {
                "uid": extra.get("uid"),
                "question": question,
                "hint": hint,
                "answers": answers,
                "prompt_w_hint": stored_pw if stored_pw is not None else _orig_prompt_text(question, hint),
                "prompt_wo_hint": stored_pwo if stored_pwo is not None else _orig_prompt_text(question, ""),
                "raw_output_w_hint": extra.get("raw_output_w_hint") or "",
                "raw_output_wo_hint": extra.get("raw_output_wo_hint") or "",
                "answer_w_hint": extra.get("answer_w_hint"),
                "answer_wo_hint": extra.get("answer_wo_hint"),
                "w_h_c": extra.get("w_h_c"),
                "wo_h_c": extra.get("wo_h_c"),
            }
        )
    return orig


def _compare_pair(
    results_a: List[Dict[str, Any]],
    results_b: List[Dict[str, Any]],
    label_a: str,
    label_b: str,
) -> None:
    """Compare two result lists element-wise and print a summary table."""
    total = min(len(results_a), len(results_b))
    full_w_match = 0
    full_wo_match = 0
    full_both_match = 0
    ans_w_match = 0
    ans_wo_match = 0
    ans_both_match = 0
    prompt_w_match = 0
    prompt_wo_match = 0
    prompt_both_match = 0
    diffs: List[Dict[str, Any]] = []
    prompt_diffs: List[Dict[str, Any]] = []

    for idx in range(total):
        ra = results_a[idx]
        rb = results_b[idx]

        # ── prompt comparison ──
        pw = ra.get("prompt_w_hint", "") == rb.get("prompt_w_hint", "")
        pwo = ra.get("prompt_wo_hint", "") == rb.get("prompt_wo_hint", "")
        if pw:
            prompt_w_match += 1
        if pwo:
            prompt_wo_match += 1
        if pw and pwo:
            prompt_both_match += 1
        if not (pw and pwo):
            diff_entry: Dict[str, Any] = {
                "index": idx,
                "uid": ra.get("uid"),
                "prompt_w_match": pw,
                "prompt_wo_match": pwo,
            }
            if not pw:
                diff_entry[f"{label_a}_prompt_w_hint"] = (ra.get("prompt_w_hint") or "")[-500:]
                diff_entry[f"{label_b}_prompt_w_hint"] = (rb.get("prompt_w_hint") or "")[-500:]
            if not pwo:
                diff_entry[f"{label_a}_prompt_wo_hint"] = (ra.get("prompt_wo_hint") or "")[-500:]
                diff_entry[f"{label_b}_prompt_wo_hint"] = (rb.get("prompt_wo_hint") or "")[-500:]
            prompt_diffs.append(diff_entry)

        # ── full output comparison ──
        fw = ra.get("raw_output_w_hint", "") == rb.get("raw_output_w_hint", "")
        fwo = ra.get("raw_output_wo_hint", "") == rb.get("raw_output_wo_hint", "")
        if fw:
            full_w_match += 1
        if fwo:
            full_wo_match += 1
        if fw and fwo:
            full_both_match += 1

        # ── extracted answer comparison ──
        aw = _normalize_answer(str(ra.get("answer_w_hint") or "")) == _normalize_answer(str(rb.get("answer_w_hint") or ""))
        awo = _normalize_answer(str(ra.get("answer_wo_hint") or "")) == _normalize_answer(str(rb.get("answer_wo_hint") or ""))
        if aw:
            ans_w_match += 1
        if awo:
            ans_wo_match += 1
        if aw and awo:
            ans_both_match += 1

        if not (fw and fwo):
            diffs.append(
                {
                    "index": idx,
                    "uid": ra.get("uid"),
                    "full_w_match": fw,
                    "full_wo_match": fwo,
                    "ans_w_match": aw,
                    "ans_wo_match": awo,
                    f"{label_a}_w_h_c": ra.get("w_h_c"),
                    f"{label_b}_w_h_c": rb.get("w_h_c"),
                    f"{label_a}_wo_h_c": ra.get("wo_h_c"),
                    f"{label_b}_wo_h_c": rb.get("wo_h_c"),
                    f"{label_a}_raw_w_hint": (ra.get("raw_output_w_hint") or "")[:200],
                    f"{label_b}_raw_w_hint": (rb.get("raw_output_w_hint") or "")[:200],
                    f"{label_a}_raw_wo_hint": (ra.get("raw_output_wo_hint") or "")[:200],
                    f"{label_b}_raw_wo_hint": (rb.get("raw_output_wo_hint") or "")[:200],
                }
            )

    print(f"\n{'=' * 60}")
    print(f"  [{label_a} ↔ {label_b}]  total={total}")
    print(f"  PROMPT match:")
    print(f"    w_hint:   {prompt_w_match}/{total}  mismatch={total - prompt_w_match}/{total}")
    print(f"    wo_hint:  {prompt_wo_match}/{total}  mismatch={total - prompt_wo_match}/{total}")
    print(f"    both:     {prompt_both_match}/{total}")
    print(f"  FULL OUTPUT match:")
    print(f"    w_hint:   {full_w_match}/{total}  mismatch={total - full_w_match}/{total}")
    print(f"    wo_hint:  {full_wo_match}/{total}  mismatch={total - full_wo_match}/{total}")
    print(f"    both:     {full_both_match}/{total}")
    print(f"  EXTRACTED ANSWER match:")
    print(f"    w_hint:   {ans_w_match}/{total}  mismatch={total - ans_w_match}/{total}")
    print(f"    wo_hint:  {ans_wo_match}/{total}  mismatch={total - ans_wo_match}/{total}")
    print(f"    both:     {ans_both_match}/{total}")

    a_whc = sum(1 for r in results_a[:total] if r.get("w_h_c") is True)
    a_wohc = sum(1 for r in results_a[:total] if r.get("wo_h_c") is True)
    b_whc = sum(1 for r in results_b[:total] if r.get("w_h_c") is True)
    b_wohc = sum(1 for r in results_b[:total] if r.get("wo_h_c") is True)
    print(f"  ACCURACY:")
    print(f"    {label_a:8s}  w_h_c_true={a_whc}  wo_h_c_true={a_wohc}")
    print(f"    {label_b:8s}  w_h_c_true={b_whc}  wo_h_c_true={b_wohc}")
    print(f"{'=' * 60}")

    if prompt_diffs:
        print(f"\n  [{label_a} ↔ {label_b}] {len(prompt_diffs)} sample(s) with PROMPT diffs:")
        for d in prompt_diffs:
            print(json.dumps(d, ensure_ascii=False))

    if diffs:
        print(f"\n  [{label_a} ↔ {label_b}] {len(diffs)} sample(s) with full-output diffs:")
        for d in diffs:
            print(json.dumps(d, ensure_ascii=False))


def _run_compare() -> None:
    """Load run1, run2 results from JSONL and compare against originals."""
    run1_results = _load_results(RUN1_OUTPUT)
    run2_results = _load_results(RUN2_OUTPUT)
    orig_results = _extract_orig()
    print(f"[compare] run1={len(run1_results)}  run2={len(run2_results)}  orig={len(orig_results)}")
    _compare_pair(run1_results, orig_results, "run1", "orig")
    _compare_pair(run2_results, orig_results, "run2", "orig")
    _compare_pair(run1_results, run2_results, "run1", "run2")


# ═══════════════════════════════════════════════════════════════════════
# MODE: (default) orchestrator — spawn subprocesses for run1 & run2
# ═══════════════════════════════════════════════════════════════════════
def _run_orchestrator() -> None:
    """Spawn two fresh subprocesses (one per inference run), then compare."""
    script = os.path.abspath(__file__)
    env = os.environ.copy()
    env["VLLM_USE_V1"] = "0"
    # env["VLLM_ENABLE_V1_MULTIPROCESSING"] = "0"

    # for run_label, output_path in [("run1", RUN1_OUTPUT), ("run2", RUN2_OUTPUT)]:
    for run_label, output_path in [("run1", RUN1_OUTPUT)]:
        print(f"\n{'─' * 60}")
        print(f"[orchestrator] spawning subprocess for {run_label} …")
        print(f"{'─' * 60}")
        cmd = [sys.executable, script, "--mode", "infer", "--output", output_path]
        result = subprocess.run(cmd, env=env)
        if result.returncode != 0:
            print(f"[orchestrator] {run_label} subprocess failed with exit code {result.returncode}")
            sys.exit(result.returncode)
        print(f"[orchestrator] {run_label} done.\n")

    print(f"\n{'─' * 60}")
    print("[orchestrator] both runs complete, comparing …")
    print(f"{'─' * 60}")
    _run_compare()


# ═══════════════════════════════════════════════════════════════════════
# CLI
# ═══════════════════════════════════════════════════════════════════════
def main() -> None:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--mode",
        choices=["infer", "compare"],
        default=None,
        help="infer: single inference run; compare: read results and compare; "
             "(default) orchestrator: spawn run1 & run2 then compare.",
    )
    parser.add_argument(
        "--output",
        default=None,
        help="Output JSONL path (only for --mode infer).",
    )
    args = parser.parse_args()

    if args.mode == "infer":
        if not args.output:
            parser.error("--output is required for --mode infer")
        _run_infer(args.output)
    elif args.mode == "compare":
        _run_compare()
    else:
        _run_orchestrator()


if __name__ == "__main__":
    main()
